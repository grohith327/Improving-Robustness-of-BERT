# Improving the Robustness of BERT by Adversarial Training
This work showcases a defense for the recent [TextFooler paper](https://arxiv.org/pdf/1907.11932.pdf). We propose a pre-training approach to improve the robustness of the BERT model to adversarial samples generated using the TextFooler algorithm. We identify the words that have high impact on the labels and mask those words to conduct a Masked Language Model (MLM) pre-training of the model. We observe an after-attack accuracy of 7% and 66% on MR and IMDB datasets respectively after pre-training
